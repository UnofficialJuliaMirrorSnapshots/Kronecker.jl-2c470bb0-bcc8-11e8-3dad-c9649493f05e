@techreport{Sch2013,
abstract = {In this paper, we review basic properties of the Kronecker product, and give an overview of its history and applications. We then move on to introducing the symmetric Kronecker product, and we derive sev- eral of its properties. Furthermore, we show its application in finding search directions in semidefinite programming.},
author = {Sch{\"{a}}cke, Kathrin},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{a}}cke - 2013 - On the Kronecker Product.pdf:pdf},
pages = {1--35},
title = {{On the Kronecker Product}},
url = {https://www.math.uwaterloo.ca/{~}hwolkowi/henry/reports/kronthesisschaecke04.pdf},
year = {2013}
}
@article{Leskovec2008,
abstract = {How can we model networks with a mathematically tractable model that allows for rigorous analysis of network properties? Networks exhibit a long list of surprising properties: heavy tails for the degree distribution; small diameters; and densification and shrinking diameters over time. Most present network models either fail to match several of the above properties, are complicated to analyze mathematically, or both. In this paper we propose a generative model for networks that is both mathematically tractable and can generate networks that have the above mentioned properties. Our main idea is to use the Kronecker product to generate graphs that we refer to as "Kronecker graphs". First, we prove that Kronecker graphs naturally obey common network properties. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks. We then present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super- exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques. Experiments on large real and synthetic networks show that KronFit finds accurate parameters that indeed very well mimic the properties of target networks. Once fitted, the model parameters can be used to gain insights about the network structure, and the resulting synthetic graphs can be used for null- models, anonymization, extrapolations, and graph summarization.},
archivePrefix = {arXiv},
arxivId = {0812.4905},
author = {Leskovec, Jure and Chakrabarti, Deepayan and Kleinberg, Jon and Faloutsos, Christos and Ghahramani, Zoubin},
eprint = {0812.4905},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Leskovec et al. - 2008 - Kronecker graphs an approach to modeling networks.pdf:pdf},
isbn = {1532-4435},
issn = {0012365X},
journal = {Journal of Machine Learning Research},
keywords = {Kronecker graphs,graph genera-tors,graph mining,network analysis,network evolution,network models,social networks},
pages = {985--1042},
pmid = {17746742},
title = {{Kronecker graphs: an approach to modeling networks}},
url = {https://cs.stanford.edu/{~}jure/pubs/kronecker-jmlr10.pdf http://arxiv.org/abs/0812.4905},
volume = {11},
year = {2008}
}
@article{Stock2017tskrr,
author = {Stock, Michiel and Pahikkala, Tapio and Airola, Antti and {De Baets}, Bernard and Waegeman, Willem},
doi = {10.1162/neco_a_01096},
journal = {Neural Computation},
number = {8},
pages = {2245--2283},
title = {{A comparative study of pairwise learning methods based on kernel ridge regression}},
volume = {30},
year = {2018}
}
@article{Airola2017genvectric,
abstract = {Kronecker product kernel provides the standard approach in the kernel methods literature for learning from pair-input data, where both data points and prediction tasks have their own feature representations. The methods allow simultaneous generalization to both new tasks and data unobserved in the training sample, a setting known as zero-shot or zero-data learning. Such a setting occurs in numerous applications, including drug-target interaction prediction, collaborative filtering and information retrieval. Efficient training algorithms based on the so-called vec trick, that makes use of the special structure of the Kronecker product, are known for the case where the output matrix for the training sample is fully observed, i.e. the correct output for each data point-task combination is available. In this work we generalize these results, proposing an efficient algorithm for sampled Kronecker product multiplication, where only a subset of the full Kronecker product, that corresponds to the training sample, is computed. This allows us to derive a general framework for training Kronecker kernel methods, as specific examples we implement Kronecker ridge regression and support vector machine algorithms. Experimental results demonstrate that the proposed approach leads to accurate models, while allowing order of magnitude improvements in training and prediction time.},
archivePrefix = {arXiv},
arxivId = {1601.01507},
author = {Airola, Antti and Pahikkala, Tapio},
doi = {10.1109/TNNLS.2017.2727545},
eprint = {1601.01507},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Airola, Pahikkala - 2017 - Fast Kronecker product kernel methods via sampled vec trick.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Index Terms kernel methods,Kronecker product kernel,bipartite graph learning,kronecker,linear algebra,ridge regression,support vector machine,transfer learning,zero-shot learning},
mendeley-tags = {kronecker,linear algebra},
number = {8},
pages = {3374--3387},
title = {{Fast Kronecker product kernel methods via generalized vec trick}},
url = {https://arxiv.org/pdf/1601.01507.pdf http://arxiv.org/abs/1601.01507},
volume = {29},
year = {2018}
}
@article{VanLoan2000,
abstract = {The Kronecker product has a rich and very pleasing algebra that supports a wide range of fast, elegant, and practical algorithms. Several trends in scientific computing suggest that this important matrix operation will have an increasingly greater role to play in the future. First, the application areas where Kronecker products abound are all thriving. These include signal processing, image processing, semidefinite programming, and quantum computing. Second, sparse factorizations and Kronecker products are proving to be a very effective way to look at fast linear transforms. Researchers have taken the Kronecker methodology as developed for the fast Fourier transform and used it to build exciting alternatives. Third, as computers get more powerful, researchers are more willing to entertain problems of high dimension and this leads to Kronecker products whenever low-dimension techniques are “tensored” together.},
author = {{Van Loan}, Charles F.},
doi = {10.1016/S0377-0427(00)00393-9},
file = {:Users/michielstock/Library/Application Support/Mendeley Desktop/Downloaded/Van Loan - 2000 - The ubiquitous Kronecker product.pdf:pdf},
isbn = {0377-0427},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
number = {1-2},
pages = {85--100},
title = {{The ubiquitous Kronecker product}},
url = {http://www.sciencedirect.com/science/article/pii/S0377042700003939},
volume = {123},
year = {2000}
}
